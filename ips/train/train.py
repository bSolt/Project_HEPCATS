# Program for training the neural network classifier


## Imports

import os, argparse
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from ips_helper import *
# from PIL import Image
# import scipy.stats as stats

DIRECTORY = "./winter_data/png/"


## Set up the feature detector
"""
# Pre-Trained Network Setup & Feature Extraction
The next code block downloads the Pre-trained network configuration and weights.  
Luckily the chosen network is included in keras, so this process if fairly straightforward. 
"""
inshape = (256,256,3)

ptdnn = tf.keras.applications.Xception(
	weights='imagenet',
	include_top=False,
	input_shape=inshape)

feature_shape = ptdnn.output_shape

## Set up the classifier
"""
# Building the Classifier Network
Next we must implement our own classifier network to be trained on the data. 
Luckily, using Keras functions, this processes is fairly straightforward.

The next code block sets up and trains the classifier network. This simple classifier 
network only have input, hidden layer, dropout, and output layers. The input layer 
corresponds to the the feature array generated by the feature detector network. 
These features are used to determine the values of the hidden layer which has 256 neurons in one dimension. 
The following layer to this is a dropout layer. this dropout unit randomly ignores half of the values from 
the hidden layer. This is useful for preventing overfitting. The output layer is just a single neuron with a 
sigmoid activation. A sigmoid activation is used here because its output can be interpreted as a probability
that the image is a positive case.

Other options specified here are in the compile method of the code. First, the model was copiled and several 
options were chosen. The chosen optimizer is known as RMSprop which is known to have very good learning 
characteristics. The `lr` variabile passed to this stands for learning rate and simply paramaterizes how 
quickly the network attempts to change weight values by during training. The loss function was specified 
as a binary, cross-entropy loss becase this is also known to yield good learning values. Metrics that will 
be recorded are specified as accuracy, recall, and F1 score.
In the last line, training settings defined above are passed to the `fit()` method. Namely a batch size of 
32 images and a total epoch count which is specified by the command line argument.
"""

classifier = tf.keras.models.Sequential(name='Classifier')
# The classifier input is dense, or fully connected
classifier.add(tf.keras.layers.Dense(256, activation='relu',
  input_shape=feature_shape[1:]))
# The flatten layer reduces the dimensions of the output
classifier.add(tf.keras.layers.Flatten())
# Dropout layer prevents overfitting
classifier.add(tf.keras.layers.Dropout(0.5))
# Output layer is a single neuron sigmoid
classifier.add(tf.keras.layers.Dense(1, activation='sigmoid'))

"""
This model will implement the combined model which detects image features 
and classifies the images in one pass. This will be the model which is actually
used for the aurora identification
"""
model = tf.keras.models.Sequential()
#First get the features from ptdnn
model.add(ptdnn)
#Then classify
model.add(classifier)
#ptdnn should not be trainable for now
model.layers[0].trainable=False
# Display layer information for reference
print('[INFO] Full Model Architecture:')
model.summary()

# classifier options including metrics and loss function
classifier.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),
              loss='binary_crossentropy',
              metrics=['acc',recall,f1])
model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=2e-4),
              loss='binary_crossentropy')

print('[INFO] Models compiled successfully')

"""This is the part where we load in the data and organize it into batches

The next block generates batches of training data. The feature array output by the 
feature detector network is of size 6 by 6 by 1536 for each image as an example. 

The subsequent block does the same for validation data.
"""
#define the training data options
train_datagen = ImageDataGenerator(
	rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

#batch size affects performance
batch_size = 32
#number of epochs
epochs = 200

# train_generator = train_datagen.flow_frow_directory(
	# DIRECTORY, batch_size=batch_size)

# Change this to something faster
train_generator = train_datagen.flow_from_directory(
        './winter_data/png/',
        target_size=(256, 256),
        batch_size=32,
        class_mode='binary')


history = model.fit_generator(
        train_generator,
        steps_per_epoch=2000//32,
        epochs=50)
        # validation_data=validation_generator,
        # validation_steps=800)